{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   age         workclass  fnlwgt  education  education-num  \\\n0    2         State-gov   77516  Bachelors             13   \n1    3  Self-emp-not-inc   83311  Bachelors             13   \n2    2           Private  215646    HS-grad              9   \n3    3           Private  234721       11th              7   \n4    1           Private  338409  Bachelors             13   \n5    2           Private  284582    Masters             14   \n6    3           Private  160187        9th              5   \n7    3  Self-emp-not-inc  209642    HS-grad              9   \n8    1           Private   45781    Masters             14   \n9    2           Private  159449  Bachelors             13   \n\n          marital-status         occupation   relationship   race     sex  \\\n0          Never-married       Adm-clerical  Not-in-family  White    Male   \n1     Married-civ-spouse    Exec-managerial        Husband  White    Male   \n2               Divorced  Handlers-cleaners  Not-in-family  White    Male   \n3     Married-civ-spouse  Handlers-cleaners        Husband  Black    Male   \n4     Married-civ-spouse     Prof-specialty           Wife  Black  Female   \n5     Married-civ-spouse    Exec-managerial           Wife  White  Female   \n6  Married-spouse-absent      Other-service  Not-in-family  Black  Female   \n7     Married-civ-spouse    Exec-managerial        Husband  White    Male   \n8          Never-married     Prof-specialty  Not-in-family  White  Female   \n9     Married-civ-spouse    Exec-managerial        Husband  White    Male   \n\n   capitalgain  capitalloss  hoursperweek native-country  class  \n0            1            0             2  United-States  <=50K  \n1            0            0             0  United-States  <=50K  \n2            0            0             2  United-States  <=50K  \n3            0            0             2  United-States  <=50K  \n4            0            0             2           Cuba  <=50K  \n5            0            0             2  United-States  <=50K  \n6            0            0             0        Jamaica  <=50K  \n7            0            0             2  United-States   >50K  \n8            4            0             3  United-States   >50K  \n9            2            0             2  United-States   >50K  \n"
     ]
    }
   ],
   "source": [
    "dataset_raw = pd.read_csv(\"dataset_183_adult.csv\",header=0)\n",
    "print(dataset_raw.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring unique values in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: age\nNumber of unique values: 5\n[2 3 1 0 4]\n\n\nColumn: workclass\nNumber of unique values: 9\n['State-gov' 'Self-emp-not-inc' 'Private' 'Federal-gov' 'Local-gov' '?'\n 'Self-emp-inc' 'Without-pay' 'Never-worked']\n\n\nColumn: fnlwgt\nNumber of unique values: 28523\n[ 77516  83311 215646 ... 173449  89686 350977]\n\n\nColumn: education\nNumber of unique values: 16\n['Bachelors' 'HS-grad' '11th' 'Masters' '9th' 'Some-college' 'Assoc-acdm'\n 'Assoc-voc' '7th-8th' 'Doctorate' 'Prof-school' '5th-6th' '10th'\n '1st-4th' 'Preschool' '12th']\n\n\nColumn: education-num\nNumber of unique values: 16\n[13  9  7 14  5 10 12 11  4 16 15  3  6  2  1  8]\n\n\nColumn: marital-status\nNumber of unique values: 7\n['Never-married' 'Married-civ-spouse' 'Divorced' 'Married-spouse-absent'\n 'Separated' 'Married-AF-spouse' 'Widowed']\n\n\nColumn: occupation\nNumber of unique values: 15\n['Adm-clerical' 'Exec-managerial' 'Handlers-cleaners' 'Prof-specialty'\n 'Other-service' 'Sales' 'Craft-repair' 'Transport-moving'\n 'Farming-fishing' 'Machine-op-inspct' 'Tech-support' '?'\n 'Protective-serv' 'Armed-Forces' 'Priv-house-serv']\n\n\nColumn: relationship\nNumber of unique values: 6\n['Not-in-family' 'Husband' 'Wife' 'Own-child' 'Unmarried' 'Other-relative']\n\n\nColumn: race\nNumber of unique values: 5\n['White' 'Black' 'Asian-Pac-Islander' 'Amer-Indian-Eskimo' 'Other']\n\n\nColumn: sex\nNumber of unique values: 2\n['Male' 'Female']\n\n\nColumn: capitalgain\nNumber of unique values: 5\n[1 0 4 2 3]\n\n\nColumn: capitalloss\nNumber of unique values: 5\n[0 3 1 2 4]\n\n\nColumn: hoursperweek\nNumber of unique values: 5\n[2 0 3 4 1]\n\n\nColumn: native-country\nNumber of unique values: 42\n['United-States' 'Cuba' 'Jamaica' 'India' '?' 'Mexico' 'South'\n 'Puerto-Rico' 'Honduras' 'England' 'Canada' 'Germany' 'Iran'\n 'Philippines' 'Italy' 'Poland' 'Columbia' 'Cambodia' 'Thailand' 'Ecuador'\n 'Laos' 'Taiwan' 'Haiti' 'Portugal' 'Dominican-Republic' 'El-Salvador'\n 'France' 'Guatemala' 'China' 'Japan' 'Yugoslavia' 'Peru'\n 'Outlying-US(Guam-USVI-etc)' 'Scotland' 'Trinadad&Tobago' 'Greece'\n 'Nicaragua' 'Vietnam' 'Hong' 'Ireland' 'Hungary' 'Holand-Netherlands']\n\n\nColumn: class\nNumber of unique values: 2\n['<=50K' '>50K']\n\n\n"
     ]
    }
   ],
   "source": [
    "for col in dataset_raw.columns:\n",
    "    print(\"Column: \" + col)\n",
    "    print(\"Number of unique values: \" + str(pd.unique(dataset_raw[col]).shape[0]))\n",
    "    print(pd.unique(dataset_raw[col]))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FnlWgt: Continuous data stats\nMin: 12285\nMax: 1490400\nMean: 189664.13459727284\nVariance: 11152210185.574848\n"
     ]
    }
   ],
   "source": [
    "print(\"FnlWgt: Continuous data stats\")\n",
    "print(\"Min: \" + str(dataset_raw['fnlwgt'].min()))\n",
    "print(\"Max: \" + str(dataset_raw['fnlwgt'].max()))\n",
    "print(\"Mean: \" + str(dataset_raw['fnlwgt'].mean()))\n",
    "print(\"Variance: \" + str(dataset_raw['fnlwgt'].var()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the dataset has mostly categorical data we will need to factorize them. Age, capitalloss, capitalgain, hoursperweek and education-num are factorized so we don't need to touch them. Education and education-num have a 1-1 relationship where education num is the factorized version so we can remove that column.\n",
    "On the other hand fnlwgt is a continuous number, so depending on the algorithm we are using we will need to normalize using either z-score or min-max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Factorizing the categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "factorized_all = dataset_raw.loc[:, dataset_raw.columns != 'education'].copy()\n",
    "to_factorize = ['class','native-country', 'sex', 'race', 'relationship', 'occupation', 'marital-status', 'workclass']\n",
    "stacked = factorized_all[to_factorize].stack()\n",
    "factorized_all[to_factorize] = pd.Series(stacked.factorize()[0], index=stacked.index).unstack().rank(method='dense')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: age\nNumber of unique values: 5\n[2 3 1 0 4]\n\n\nColumn: workclass\nNumber of unique values: 9\n[1. 2. 3. 5. 6. 4. 7. 8. 9.]\n\n\nColumn: fnlwgt\nNumber of unique values: 28523\n[ 77516  83311 215646 ... 173449  89686 350977]\n\n\nColumn: education-num\nNumber of unique values: 16\n[13  9  7 14  5 10 12 11  4 16 15  3  6  2  1  8]\n\n\nColumn: marital-status\nNumber of unique values: 7\n[1. 2. 3. 4. 5. 6. 7.]\n\n\nColumn: occupation\nNumber of unique values: 15\n[ 1.  2.  3.  4.  5.  6.  8.  9. 10. 11. 12.  7. 13. 14. 15.]\n\n\nColumn: relationship\nNumber of unique values: 6\n[1. 2. 3. 4. 5. 6.]\n\n\nColumn: race\nNumber of unique values: 5\n[1. 2. 3. 4. 5.]\n\n\nColumn: sex\nNumber of unique values: 2\n[1. 2.]\n\n\nColumn: capitalgain\nNumber of unique values: 5\n[1 0 4 2 3]\n\n\nColumn: capitalloss\nNumber of unique values: 5\n[0 3 1 2 4]\n\n\nColumn: hoursperweek\nNumber of unique values: 5\n[2 0 3 4 1]\n\n\nColumn: native-country\nNumber of unique values: 42\n[ 1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17. 18.\n 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35. 36.\n 37. 38. 39. 40. 41. 42.]\n\n\nColumn: class\nNumber of unique values: 2\n[1. 2.]\n\n\n"
     ]
    }
   ],
   "source": [
    "for col in factorized_all.columns:\n",
    "    print(\"Column: \" + col)\n",
    "    print(\"Number of unique values: \" + str(pd.unique(factorized_all[col]).shape[0]))\n",
    "    print(pd.unique(factorized_all[col]))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data is factorized we need to min max normalize all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48842, 14)\n[[0.5        0.         0.04413121 0.8        0.         0.\n  0.         0.         0.         0.25       0.         0.5\n  0.         0.        ]\n [0.75       0.125      0.04805174 0.8        0.16666667 0.07142857\n  0.2        0.         0.         0.         0.         0.\n  0.         0.        ]\n [0.5        0.25       0.13758131 0.53333333 0.33333333 0.14285714\n  0.         0.         0.         0.         0.         0.5\n  0.         0.        ]\n [0.75       0.25       0.15048626 0.4        0.16666667 0.14285714\n  0.2        0.25       0.         0.         0.         0.5\n  0.         0.        ]\n [0.25       0.25       0.22063507 0.8        0.16666667 0.21428571\n  0.4        0.25       1.         0.         0.         0.5\n  0.02439024 0.        ]\n [0.5        0.25       0.18421909 0.86666667 0.16666667 0.07142857\n  0.4        0.         1.         0.         0.         0.5\n  0.         0.        ]\n [0.75       0.25       0.10006123 0.26666667 0.5        0.28571429\n  0.         0.25       1.         0.         0.         0.\n  0.04878049 0.        ]\n [0.75       0.125      0.13351938 0.53333333 0.16666667 0.07142857\n  0.2        0.         0.         0.         0.         0.5\n  0.         1.        ]\n [0.25       0.25       0.02266129 0.86666667 0.         0.21428571\n  0.         0.         1.         1.         0.         0.75\n  0.         1.        ]\n [0.5        0.25       0.09956194 0.8        0.16666667 0.07142857\n  0.2        0.         0.         0.5        0.         0.5\n  0.         1.        ]\n [0.5        0.25       0.18143311 0.6        0.16666667 0.07142857\n  0.2        0.25       0.         0.         0.         1.\n  0.         1.        ]\n [0.25       0.         0.08728144 0.8        0.16666667 0.21428571\n  0.2        0.5        0.         0.         0.         0.5\n  0.07317073 1.        ]\n [0.         0.25       0.07441031 0.8        0.         0.\n  0.6        0.         1.         0.         0.         0.25\n  0.         0.        ]\n [0.25       0.25       0.13039175 0.73333333 0.         0.35714286\n  0.         0.25       0.         0.         0.         0.75\n  0.         0.        ]\n [0.5        0.25       0.07407204 0.66666667 0.16666667 0.5\n  0.2        0.5        0.         0.         0.         0.5\n  0.09756098 1.        ]\n [0.25       0.25       0.15776986 0.2        0.16666667 0.57142857\n  0.2        0.75       0.         0.         0.         0.5\n  0.12195122 0.        ]\n [0.         0.125      0.11127077 0.53333333 0.         0.64285714\n  0.6        0.         0.         0.         0.         0.25\n  0.         0.        ]\n [0.25       0.25       0.11808215 0.53333333 0.         0.71428571\n  0.8        0.         0.         0.         0.         0.5\n  0.         0.        ]\n [0.5        0.25       0.01123187 0.4        0.16666667 0.35714286\n  0.2        0.         0.         0.         0.         0.75\n  0.         0.        ]\n [0.5        0.125      0.18935604 0.86666667 0.33333333 0.07142857\n  0.8        0.         1.         0.         0.         0.5\n  0.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "normal_min_max = factorized_all.copy().values\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "dataset_scaled = min_max_scaler.fit_transform(normal_min_max)\n",
    "print(dataset_scaled.shape)\n",
    "print(dataset_scaled[0:20,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the dataset has been factorized and normalized, we can build our models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.89543554  0.73869571  0.701705    4.64635296 -0.44199952 -0.53662057\n  -0.32304578 -0.62108266 -1.1393819   3.73196017  1.9657544   1.97384637]]\n[[3489  211]\n [ 668  517]]\n\n\n[[ 1.92143488  0.70799386  0.61698928  4.57713281 -0.46949232 -0.54824568\n  -0.30657991 -0.56167111 -1.11227278  3.74248149  1.96245776  2.02881641]]\n[[3536  211]\n [ 627  511]]\n\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.92295595  0.72845244  0.5724042   4.6555245  -0.48000053 -0.55234459\n  -0.30072097 -0.62360661 -1.1169472   3.73292237  1.89639308  2.00605045]]\n[[3496  216]\n [ 630  542]]\n\n\n[[ 1.90846906  0.70968099  0.56749797  4.58761166 -0.47336676 -0.52040874\n  -0.33160547 -0.62553811 -1.11549888  3.70362643  1.95766793  1.99691441]]\n[[3512  204]\n [ 640  528]]\n\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.91810525  0.71499693  0.6150228   4.5612632  -0.45807488 -0.51244556\n  -0.33437879 -0.63019994 -1.09801183  3.73758031  1.97990941  1.99027036]]\n[[3489  205]\n [ 646  544]]\n\n\n[[ 1.902086    0.76329928  0.59840946  4.58961277 -0.44254548 -0.54945709\n  -0.31553012 -0.62256356 -1.12141568  3.7248896   1.99318434  1.94390391]]\n[[3503  197]\n [ 635  549]]\n\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.9259821   0.70387513  0.63530044  4.61164599 -0.51531152 -0.50153855\n  -0.31981617 -0.65942112 -1.11858674  3.69900817  1.89203617  1.99513408]]\n[[3469  213]\n [ 654  548]]\n\n\n[[ 1.88607905  0.68629871  0.65887836  4.61892365 -0.41213508 -0.50537485\n  -0.31758421 -0.68087193 -1.1120942   3.70365163  1.93609642  1.9767217 ]]\n[[3554  191]\n [ 639  500]]\n\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.91536214  0.66897872  0.67071626  4.59934647 -0.42184088 -0.54277734\n  -0.30385062 -0.65002378 -1.13052646  3.71410411  1.98728699  2.00340947]]\n[[3516  221]\n [ 642  505]]\n\n\n[[ 1.92604651  0.72721285  0.53901366  4.5929849  -0.46522622 -0.53620003\n  -0.32408365 -0.63830527 -1.10364229  3.71863598  1.98039604  1.99912455]]\n[[3510  212]\n [ 631  531]]\n\n\n"
     ]
    }
   ],
   "source": [
    "kf10 = KFold(n_splits=10)\n",
    "logreg_acc = []\n",
    "logreg_pres = []\n",
    "logreg_rec = []\n",
    "logreg_err = []\n",
    "logreg_f1 = []\n",
    "for train_index, test_index in kf10.split(dataset_scaled):\n",
    "    train, test = dataset_scaled[train_index], dataset_scaled[test_index]\n",
    "    logreg = LogisticRegression(solver=\"liblinear\").fit(train[:,0:12],train[:,13])\n",
    "    predictions = logreg.predict(test[:,0:12])\n",
    "    c_matrix = confusion_matrix(test[:,13], predictions)\n",
    "    print(abs(logreg.coef_[0]))\n",
    "    print(c_matrix)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Metrics: k=10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average precision: 0.7169721193413964\nAverage recall: 0.45135440957615114\nAverage f1: 0.45135440957615114\nAverage accuracy: 0.8260717885209798\n"
     ]
    }
   ],
   "source": [
    "logreg_1 = LogisticRegression(solver=\"liblinear\")\n",
    "print(\"Logistic Regression Metrics: k=10\")\n",
    "scoring_metrics = {'precision': 'test_precision', 'recall': 'test_recall', 'f1': 'test_recall', 'accuracy': 'test_accuracy'}\n",
    "scores = cross_validate(logreg_1, dataset_scaled[:,0:12], dataset_scaled[:,13], cv=10, scoring=list(scoring_metrics.keys()), return_estimator=True)\n",
    "for metric in scoring_metrics.keys():\n",
    "    print(\"Average \" + metric + \": \" + str(sum(scores[scoring_metrics[metric]])/ len(scores[scoring_metrics[metric]])))\n",
    "log_importances = []\n",
    "for estimator in scores['estimator']:\n",
    "    feature_importance = abs(estimator.coef_[0])\n",
    "    feature_importance = 100 * (feature_importance / feature_importance.max())\n",
    "    log_importances.append(feature_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.895295175913726"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores['estimator'][0].coef_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 40.81149705,  15.9780503 ,  14.97457107, 100.        ,\n         9.53176934,  11.53346939,   6.97655703,  13.48723154,\n        24.52808499,  80.3269876 ,  42.41666731,  42.62507052]), array([ 41.99237116,  15.27239379,  13.73597998, 100.        ,\n        10.24027845,  11.94258235,   6.63836233,  12.11551681,\n        24.29629167,  81.79710235,  42.83792028,  44.19666634]), array([ 41.2884388 ,  15.79741365,  12.24060364, 100.        ,\n        10.31027596,  11.82501274,   6.47854174,  13.39624065,\n        23.9584309 ,  80.15606959,  40.69440014,  43.02856368]), array([ 41.57305242,  15.37849509,  12.40151946, 100.        ,\n        10.26605786,  11.45690773,   7.22816712,  13.62284808,\n        24.33490938,  80.58630215,  42.54029139,  43.49800111]), array([ 42.04161082,  15.66472205,  13.24419118, 100.        ,\n        10.11824709,  11.20608041,   7.35820632,  13.85165695,\n        24.06578217,  81.96629895,  43.47592246,  43.68589806]), array([ 41.46105576,  16.67053241,  13.30844946, 100.        ,\n         9.56271397,  11.862031  ,   6.87078688,  13.59596307,\n        24.44439938,  81.24603388,  43.46312112,  42.44640768]), array([ 41.70152206,  15.36723828,  13.803133  , 100.        ,\n        11.16515169,  11.06800271,   6.76916411,  14.23827106,\n        24.22594087,  79.99965374,  41.07747937,  43.29798826]), array([ 40.92176061,  14.88172902,  13.87411925, 100.        ,\n         9.05324899,  10.92896693,   7.05549825,  14.82308021,\n        24.12275323,  80.48433102,  42.06380285,  42.79240597]), array([ 41.6085339 ,  14.38220805,  14.75043628, 100.        ,\n         9.11355683,  11.74405408,   6.54854951,  14.0368962 ,\n        24.56401098,  80.58449962,  42.93905817,  43.41256789]), array([ 41.94771962,  15.89425742,  11.79346841, 100.        ,\n        10.11773296,  11.66646311,   7.09336846,  13.949091  ,\n        24.03687074,  81.06823044,  43.18246744,  43.57152951])]\n<class 'list'>\n[ 40.81149705  15.9780503   14.97457107 100.           9.53176934\n  11.53346939   6.97655703  13.48723154  24.52808499  80.3269876\n  42.41666731  42.62507052]\n<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(log_importances)\n",
    "print(type(log_importances))\n",
    "print(log_importances[0])\n",
    "print(type(log_importances[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 82.80386821,  31.2504441 ,  28.71055105, 200.        ,\n        19.77204778,  23.47605174,  13.61491936,  25.60274835,\n        48.82437666, 162.12408995,  85.25458758,  86.82173686])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_importances[0] + log_importances[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}